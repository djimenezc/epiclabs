package com.djimenezc.duplicatedUrls;

import java.io.File;
import java.util.ArrayList;
import java.util.List;

/**
 * You have a file with 10 billion of urls (one per line).
 * These are the full list of urls to be crawled by your crawler.
 * How do you determine what urlâ€™s are duplicated?
 * Assume you have just 1 GB of memory available for this
 * <p>
 * Created by david on 19/06/2016.
 */
class UrlCrawler {

  List<String> findDuplicatedLines(File file) {

    List<String> duplicatedLines = new ArrayList<>();

    return duplicatedLines;
  }
}
